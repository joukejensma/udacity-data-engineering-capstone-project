{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9339161c",
   "metadata": {},
   "source": [
    "# Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Our objective is to gain insight in tourism by finding out how many people are traveling to destinations (states), what the weather conditions (average temperature) are like at that moment in time so tourism operators can adjust their offerings accordingly. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c8c7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b0139c0863464bae075c23804f5433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Package already installed for current Spark context!\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 1110, in install_pypi_package\n",
      "    raise ValueError(\"Package already installed for current Spark context!\")\n",
      "ValueError: Package already installed for current Spark context!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# install pandas package, required for .toPandas() functionality\n",
    "sc.install_pypi_package(\"pandas==0.25.1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ebdc536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bec323b827433c8d739c2d2e883bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, to_date, col, month, year, dayofmonth, split, format_string, abs, isnan, when, count, substring, length, regexp_extract, monotonically_increasing_id\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import pyspark.sql.types as t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ab16503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703b071259e94c8b8cb288d4d76dc003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# settings, specify the s3 bucket name on which to read data from and store data on\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read('settings.cfg')\n",
    "\n",
    "# ensure that the environment variables are set before the spark session is started, otherwise s3 cannot be accessed\n",
    "# os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "# s3_bucket=config['AWS']['AWS_S3_BUCKET_LOC']\n",
    "s3_bucket='jjudacitydatalake'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3421b8e",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "We want to gain insight in tourism in the United States. Questions like, how many people are traveling to destinations and what are the weather conditions like at the arrival location. The goal is for tourism operators to gain insight in why people travel there so they can adjust their tourism offerings. \n",
    "\n",
    "We use the immigration dataset provided by Udacity (originally from the US National Tourism and Trade Office) which can be found here: https://travel.trade.gov/research/reports/i94/historical/2016.html. \n",
    "\n",
    "Our goal is to run a one-time analysis on the full (i.e. the whole year) dataset. We want to match the immigration data to historical temperatures.\n",
    "One particular example is to count how many people are arriving in a certain state per time period (year, month) and what the expected historical average temperature is like at that period in time.\n",
    "\n",
    "We'll store the immigration data and the temperature data on S3 with a suitable partitioning for efficient processing. The processing itself will be done via Apache Spark on an EMR cluster. Results will be written back to S3.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### Immigration data\n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "We use the immigration dataset provided by Udacity (originally from the US National Tourism and Trade Office) which can be found here: https://travel.trade.gov/research/reports/i94/historical/2016.html. The US National Tourism and Trade Office provides this dataset to the public for third parties to gain useful insights. Each entry in the dataset is an arrival into the USA with additional data provided (such as gender, date of arrival, airline, purposes of visit (we filter on tourism/pleasure).\n",
    "\n",
    "The Immigration data has been provided by a separate disk on the Udacity workspace. I have read this data in with pandas and re-written it without modifications as parquet files in the i94parquet folder on my S3 folder.\n",
    "\n",
    "##### Temperature data\n",
    "This dataset was provided by Udacity and is originally sourced from Kaggle. More information can be found here: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6bb63d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51cb4c599224633a5eb5d4be47b3937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows read: 6540562"
     ]
    }
   ],
   "source": [
    "def readMultipleParquet(listPaths):\n",
    "    \"\"\"\n",
    "    Given a list of paths, return Spark Dataframe for processing\n",
    "    \"\"\"\n",
    "    sc.setJobGroup(\"Read\", \"Reading multiple parquet files\")\n",
    "    return spark.read.parquet(*listPaths)\n",
    "\n",
    "listPaths = [f's3://{s3_bucket}/capstone/staging/i94_parquet/i94_apr16_sub.sas7bdat', f's3://{s3_bucket}/capstone/staging/i94_parquet/i94_may16_sub.sas7bdat']\n",
    "\n",
    "immigration_staging = readMultipleParquet(listPaths)\n",
    "\n",
    "print(f\"Number of rows read: {immigration_staging.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920598d6",
   "metadata": {},
   "source": [
    "## Examine the raw immigration data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f10f2a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edb14ec19774e9bb9524ad42156ded7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid    |i94yr |i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto |gender|insnum|airline|admnum        |fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|1252973.0|2016.0|5.0   |127.0 |127.0 |MIA    |20581.0|1.0    |FL     |20610.0|49.0  |1.0    |1.0  |20160507|BCH     |null |G      |N      |null   |M      |1967.0 |05062018|M     |null  |AA     |9.563070103E10|01466|E2      |\n",
      "|1252974.0|2016.0|5.0   |127.0 |127.0 |MIA    |20581.0|1.0    |FL     |20587.0|30.0  |2.0    |1.0  |20160507|BCH     |null |G      |O      |null   |M      |1986.0 |11062016|F     |null  |BA     |9.567985723E10|00209|B2      |\n",
      "|1252975.0|2016.0|5.0   |127.0 |127.0 |MIA    |20581.0|1.0    |FL     |20590.0|61.0  |2.0    |1.0  |20160507|BCH     |null |G      |O      |null   |M      |1955.0 |11062016|M     |null  |LH     |9.565351503E10|00462|B2      |\n",
      "|1252976.0|2016.0|5.0   |127.0 |127.0 |MIA    |20581.0|1.0    |FL     |20590.0|37.0  |2.0    |1.0  |20160507|BCH     |null |G      |O      |null   |M      |1979.0 |11062016|F     |null  |LH     |9.565327023E10|00462|B2      |\n",
      "|1252977.0|2016.0|5.0   |127.0 |127.0 |MIA    |20581.0|1.0    |FL     |20592.0|66.0  |2.0    |1.0  |20160507|BCH     |null |G      |O      |null   |M      |1950.0 |11062016|M     |null  |AF     |9.567705223E10|00090|B2      |\n",
      "|1252978.0|2016.0|5.0   |127.0 |127.0 |MIA    |20581.0|1.0    |FL     |20592.0|61.0  |2.0    |1.0  |20160507|BCH     |null |G      |O      |null   |M      |1955.0 |11062016|M     |null  |AF     |9.567687803E10|00090|B2      |\n",
      "|1252979.0|2016.0|5.0   |127.0 |127.0 |MIA    |20581.0|1.0    |FL     |20594.0|49.0  |2.0    |1.0  |20160507|BCH     |null |G      |O      |null   |M      |1967.0 |11062016|M     |null  |BA     |9.568058413E10|00209|B2      |\n",
      "|1252980.0|2016.0|5.0   |127.0 |127.0 |MIA    |20581.0|1.0    |FL     |20594.0|40.0  |2.0    |1.0  |20160507|BCH     |null |G      |O      |null   |M      |1976.0 |11062016|F     |null  |BA     |9.568068313E10|00209|B2      |\n",
      "|1252981.0|2016.0|5.0   |127.0 |127.0 |MIA    |20581.0|1.0    |FL     |20595.0|28.0  |2.0    |1.0  |20160507|BCH     |null |G      |O      |null   |M      |1988.0 |11062016|F     |null  |TK     |9.568577253E10|00077|B2      |\n",
      "|1252982.0|2016.0|5.0   |127.0 |127.0 |MIA    |20581.0|1.0    |FL     |20597.0|28.0  |2.0    |1.0  |20160507|BCH     |null |G      |O      |null   |M      |1988.0 |11062016|F     |null  |AF     |9.567528403E10|00090|B2      |\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "immigration_staging.limit(10).show(truncate=False)\n",
    "\n",
    "immigration_staging.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1f726",
   "metadata": {},
   "source": [
    "# Dataset enhancement\n",
    "\n",
    "### This is part of Step 2: Explore and Assess the Data\n",
    "Identify data quality issues, like missing values, duplicate data, etc. & Document steps necessary to clean the data.\n",
    "\n",
    "## Tourism filtering\n",
    "We are interested in tourism data so from the data description we find we have to filter on i94visa = 2 (Pleasure).\n",
    "\n",
    "<code>\n",
    "/* I94VISA - Visa codes collapsed into three categories:\n",
    "   1 = Business\n",
    "   2 = Pleasure\n",
    "   3 = Student\n",
    "*/\n",
    "</code>\n",
    "\n",
    "## Adding useful date fields\n",
    "The arrdate and depdate fields are double. They represent the number of days since 1-1-1960.\n",
    "We add columns arrdate_dt, depdate_dt (to parse them as proper datetime objects) as well as day of month, month and year columns which can be extracted from arrdate_dt.\n",
    "\n",
    "## Fixing US states\n",
    "The US contains 50 states. The field i94addr can contain invalid data (i.e. entries outside the applicable list of 50 states). If this happens we replace the value with 'other'.\n",
    "We also replace values null with other.\n",
    "\n",
    "## Dropping duplicates\n",
    "We drop duplicate rows in the immigration dataset. Each row ought to be unique.\n",
    "\n",
    "## Gender\n",
    "Sometimes gender is null. As there's no way of enriching the data we substitute the null value with 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a7a14cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d54f6a942154d87a682ed48593a838a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in final selected dataset: 5388905"
     ]
    }
   ],
   "source": [
    "def sasDateToDatetime(sasdate):\n",
    "    \"\"\"\n",
    "    Given a spark column which specifies the number of days since 1960, return the datetime object\n",
    "    \"\"\"\n",
    "    return None if sasdate == None else datetime.strptime('1960-01-01', \"%Y-%m-%d\") + timedelta(sasdate)\n",
    "\n",
    "valid_us_states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "\n",
    "sasdate_udf = udf(sasDateToDatetime, t.DateType())\n",
    "imm = immigration_staging.\\\n",
    "    withColumn('arrdate_dt', sasdate_udf('arrdate')).\\\n",
    "    withColumn('depdate_dt', sasdate_udf('depdate')).\\\n",
    "    withColumn('arrdate_dayofmonth', dayofmonth(col('arrdate_dt'))).\\\n",
    "    withColumn('arrdate_month', month(col('arrdate_dt'))).\\\n",
    "    withColumn('arrdate_year', year(col('arrdate_dt'))).\\\n",
    "    withColumn('state', when(~col('i94addr').isin(valid_us_states), 'other').otherwise(col('i94addr'))).\\\n",
    "    fillna('other', subset='state').\\\n",
    "    fillna('unknown', subset='gender').\\\n",
    "    dropDuplicates().\\\n",
    "    select('i94port', 'biryear', 'gender', 'airline', 'i94visa', 'arrdate_dt', 'depdate_dt', 'arrdate_dayofmonth', 'arrdate_month', 'arrdate_year', 'state').\\\n",
    "    filter(col('i94visa') == 2).\\\n",
    "    withColumn('id_imm', monotonically_increasing_id())\n",
    "\n",
    "print(f\"Number of rows in final selected dataset: {imm.count()}\")\n",
    "\n",
    "# dataframe.filter(~dataframe.column.isin(array))\n",
    "#     withColumn(\"state\", when(immigration_staging[\"i94addr\"] not in valid_us_states, 'other').otherwise(immigration_staging[\"i94addr\"])).\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398d3ef",
   "metadata": {},
   "source": [
    "## Examine subset of immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb9777b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e9e34b6073442c94a1ef0df3b5a2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+----------+----------+------------------+-------------+------------+-----+----------+\n",
      "|i94port|biryear|gender |airline|i94visa|arrdate_dt|depdate_dt|arrdate_dayofmonth|arrdate_month|arrdate_year|state|id_imm    |\n",
      "+-------+-------+-------+-------+-------+----------+----------+------------------+-------------+------------+-----+----------+\n",
      "|LVG    |1951.0 |M      |VS     |2.0    |2016-05-09|2016-05-22|9                 |5            |2016        |NV   |8589934592|\n",
      "|ORL    |1994.0 |unknown|BA     |2.0    |2016-05-09|2016-05-23|9                 |5            |2016        |FL   |8589934593|\n",
      "|MIA    |1994.0 |M      |BA     |2.0    |2016-05-09|2016-05-14|9                 |5            |2016        |FL   |8589934594|\n",
      "|SFR    |1986.0 |unknown|BA     |2.0    |2016-05-09|2016-05-31|9                 |5            |2016        |CA   |8589934595|\n",
      "|PIT    |1992.0 |M      |ZX     |2.0    |2016-05-09|2016-05-27|9                 |5            |2016        |PA   |8589934596|\n",
      "|NYC    |1964.0 |F      |LH     |2.0    |2016-05-09|2016-05-15|9                 |5            |2016        |NY   |8589934597|\n",
      "|LOS    |1963.0 |unknown|AB     |2.0    |2016-05-09|2016-05-23|9                 |5            |2016        |CA   |8589934598|\n",
      "|HHW    |1979.0 |F      |DL     |2.0    |2016-05-09|2016-05-12|9                 |5            |2016        |HI   |8589934599|\n",
      "|HHW    |1982.0 |F      |DL     |2.0    |2016-05-09|2016-05-12|9                 |5            |2016        |HI   |8589934600|\n",
      "|HHW    |1978.0 |F      |HA     |2.0    |2016-05-09|2016-05-13|9                 |5            |2016        |HI   |8589934601|\n",
      "+-------+-------+-------+-------+-------+----------+----------+------------------+-------------+------------+-----+----------+\n",
      "\n",
      "root\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- gender: string (nullable = false)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- arrdate_dt: date (nullable = true)\n",
      " |-- depdate_dt: date (nullable = true)\n",
      " |-- arrdate_dayofmonth: integer (nullable = true)\n",
      " |-- arrdate_month: integer (nullable = true)\n",
      " |-- arrdate_year: integer (nullable = true)\n",
      " |-- state: string (nullable = false)\n",
      " |-- id_imm: long (nullable = false)"
     ]
    }
   ],
   "source": [
    "imm.limit(10).show(truncate=False)\n",
    "\n",
    "imm.printSchema()\n",
    "\n",
    "# result = imm.select('state').distinct().collect()\n",
    "# print(f'All distinct states are: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f349dc",
   "metadata": {},
   "source": [
    "# Read in immigration data\n",
    "\n",
    "Explore data skewness, find partition/clustering keys in order to parallelize processing\n",
    "\n",
    "I have a gut feeling that the number of people arriving each month and day of month ought to be relatively constant. Let's validate this assumption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "082035b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cb7a453bf04e4d9f42b863aae72ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    month  day  immnum\n",
      "0       4    1   95254\n",
      "1       4    2   83883\n",
      "2       4    3   66613\n",
      "3       4    4   69904\n",
      "4       4    5   74432\n",
      "..    ...  ...     ...\n",
      "56      5   27  107975\n",
      "57      5   28  104771\n",
      "58      5   29   75577\n",
      "59      5   30   64529\n",
      "60      5   31   69553\n",
      "\n",
      "[61 rows x 3 columns]"
     ]
    }
   ],
   "source": [
    "imm.createOrReplaceTempView(\"immdata\")\n",
    "dataSkewQuery = spark.sql(\"\"\"\n",
    "select arrdate_month as month, arrdate_dayofmonth as day, count(*) as immnum\n",
    "from immdata\n",
    "group by month, day\n",
    "order by month asc, day asc\n",
    "\"\"\")\n",
    "dataSkewQuery.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9ef13",
   "metadata": {},
   "source": [
    "## Examine seasonality in the data, how many visitors visit the US each month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "779b2511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9ec8e889464731b3f2af49b9e2f6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   month   immnum\n",
      "0      4  2530868\n",
      "1      5  2858037"
     ]
    }
   ],
   "source": [
    "imm.createOrReplaceTempView(\"immdata\")\n",
    "seasonality_table = spark.sql(\"\"\"\n",
    "select arrdate_month as month, count(*) as immnum\n",
    "from immdata\n",
    "group by month\n",
    "order by month asc\n",
    "\"\"\")\n",
    "seasonality_table.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93763b62",
   "metadata": {},
   "source": [
    "# Read in temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53845c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7a8a6a70694d6fa9eb86979a839fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|dt        |AverageTemperature|AverageTemperatureUncertainty|City |Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|6.068             |1.7369999999999999           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1743-12-01|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-01-01|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-02-01|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-03-01|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-04-01|5.7879999999999985|3.6239999999999997           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-05-01|10.644            |1.2830000000000001           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-06-01|14.050999999999998|1.347                        |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-07-01|16.082            |1.396                        |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-08-01|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+"
     ]
    }
   ],
   "source": [
    "temp_staging = spark.read.option(\"header\", \"true\").csv(f's3://{s3_bucket}/capstone/staging/temperature_data/GlobalLandTemperaturesByCity.csv')\n",
    "temp_staging.limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75d66896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8b453e7dbe4e31bcf5ae630bd11478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_temp = temp_staging.\\\n",
    "    filter(col('Country') == 'United States').\\\n",
    "    select(to_date(col(\"dt\"),\"yyyy-MM-dd\").alias(\"dt\"), 'AverageTemperature', 'City', 'Country', 'Latitude', 'Longitude').\\\n",
    "    withColumn('dayofmonth', dayofmonth(col('dt'))).\\\n",
    "    withColumn('month', month(col('dt'))).\\\n",
    "    withColumn('year', year(col('dt'))).\\\n",
    "    withColumn(\"latitude_rounded\", format_string(\"%.0f\", regexp_extract(col('Latitude'), '\\d+.\\d+', 0).cast(t.DoubleType()))).\\\n",
    "    withColumn(\"longitude_rounded\", format_string(\"%.0f\", regexp_extract(col('Longitude'), '\\d+.\\d+', 0).cast(t.DoubleType()))).\\\n",
    "    dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "547d1e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b10c983193e46d091c1c5baa7d9b9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+----+------------------+-------------+\n",
      "|dayofmonth|month|lat|long|AvgTemp           |id_temp_coord|\n",
      "+----------+-----+---+----+------------------+-------------+\n",
      "|1         |1    |27 |81  |17.598033333333333|0            |\n",
      "|1         |2    |27 |81  |18.65918828451882 |1            |\n",
      "|1         |3    |27 |81  |20.510491666666674|2            |\n",
      "|1         |4    |27 |81  |22.596529166666656|3            |\n",
      "|1         |5    |27 |81  |25.009309623430973|4            |\n",
      "|1         |6    |27 |81  |27.00768049792531 |5            |\n",
      "|1         |7    |27 |81  |27.85293775933608 |6            |\n",
      "|1         |8    |27 |81  |27.92431535269709 |7            |\n",
      "|1         |9    |27 |81  |26.861252100840332|8            |\n",
      "|1         |10   |27 |81  |23.967126582278475|9            |\n",
      "|1         |11   |27 |81  |20.727659663865523|10           |\n",
      "|1         |12   |27 |81  |18.027008403361332|11           |\n",
      "|1         |1    |27 |82  |18.587668103448276|12           |\n",
      "|1         |2    |27 |82  |18.725623376623375|13           |\n",
      "|1         |3    |27 |82  |19.821219827586205|14           |\n",
      "|1         |4    |27 |82  |21.626489270386273|15           |\n",
      "|1         |5    |27 |82  |24.153780172413803|16           |\n",
      "|1         |6    |27 |82  |26.069188841201726|17           |\n",
      "|1         |7    |27 |82  |26.762415254237297|18           |\n",
      "|1         |8    |27 |82  |27.156209401709397|19           |\n",
      "+----------+-----+---+----+------------------+-------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "\n",
    "df_temp.createOrReplaceTempView(\"tempdata_coord\")\n",
    "temp_table = spark.sql(\"\"\"\n",
    "select dayofmonth, month, latitude_rounded as lat, longitude_rounded as long, avg(AverageTemperature) as AvgTemp\n",
    "from tempdata_coord\n",
    "group by lat, long, month, dayofmonth\n",
    "order by lat asc, long asc, month asc, dayofmonth asc\n",
    "\"\"\")\n",
    "\n",
    "temp_table = temp_table.withColumn(\"id_temp_coord\", monotonically_increasing_id())\n",
    "temp_table.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f27639",
   "metadata": {},
   "source": [
    "# Read in airport code data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b8641c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1625eb4d9fc4a65ac0628d10ec136b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "airport_codes_staging = spark.read.option(\"header\", \"true\").csv(f's3://{s3_bucket}/capstone/staging/airportcodes_data/airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439d255",
   "metadata": {},
   "source": [
    "# Mapping coordinates to states\n",
    "We introduce a bit of noise by rounding the lat/long coordinates to 0 decimals. This means that each lat, long pair can have multiple states, which is undesirable.\n",
    "\n",
    "Below we count the number of states contained within each lat/long pair and select the maximum as being the most representative state for each lat/long coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae40d27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e562d3ad2e740b58afee0ecac3e8152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+--------------+\n",
      "|latitude|longitude|state|id_state_coord|\n",
      "+--------+---------+-----+--------------+\n",
      "|0       |0        |other|0             |\n",
      "|0       |1        |PA   |1             |\n",
      "|1       |0        |NC   |2             |\n",
      "|1       |1        |OK   |3             |\n",
      "|1       |1        |PA   |4             |\n",
      "|1       |1        |other|5             |\n",
      "|1       |3        |NY   |6             |\n",
      "|1       |3        |other|7             |\n",
      "|12      |8        |CA   |8             |\n",
      "|19      |155      |HI   |9             |\n",
      "|19      |156      |HI   |10            |\n",
      "|19      |17       |other|11            |\n",
      "|2       |2        |other|12            |\n",
      "|20      |155      |HI   |13            |\n",
      "|20      |156      |HI   |14            |\n",
      "|21      |156      |HI   |15            |\n",
      "|21      |157      |HI   |16            |\n",
      "|21      |158      |HI   |17            |\n",
      "|22      |158      |HI   |18            |\n",
      "|22      |159      |HI   |19            |\n",
      "+--------+---------+-----+--------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# coordinates are specified in [longitude, latitude]\n",
    "coordinates_split = split(airport_codes_staging['coordinates'], ',')\n",
    "region_split = split(airport_codes_staging['iso_region'], '-')\n",
    "\n",
    "df_airportcodes = airport_codes_staging.\\\n",
    "                    filter(col('iso_country') == 'US').\\\n",
    "                    withColumn(\"latitude\", format_string(\"%.0f\", abs(coordinates_split.getItem(1).cast(t.DoubleType())))).\\\n",
    "                    withColumn(\"longitude\", format_string(\"%.0f\", abs(coordinates_split.getItem(0).cast(t.DoubleType())))).\\\n",
    "                    withColumn(\"state\", region_split.getItem(1)).\\\n",
    "                    withColumn('state', when(~col('state').isin(valid_us_states), 'other').otherwise(col('state'))).\\\n",
    "                    fillna('other', subset='state')\n",
    "\n",
    "df_airportcodes.createOrReplaceTempView(\"aircodes\")\n",
    "# count the number of states for each lat/long pair\n",
    "aircode_table1 = spark.sql(\"\"\"\n",
    "select latitude, longitude, state, count(state) as num\n",
    "from aircodes\n",
    "group by latitude, longitude, state\n",
    "order by latitude, longitude, state\n",
    "\"\"\")\n",
    "# aircode_table1.show(truncate=False)\n",
    "\n",
    "df_airportcodes.createOrReplaceTempView(\"aircodes\")\n",
    "# determine the maximum count per lat/long pair\n",
    "aircode_table2 = spark.sql(\"\"\"\n",
    "select latitude as lat, longitude as long, max(num) as maxPerLatLong from (\n",
    "    select latitude, longitude, state, count(state) as num\n",
    "    from aircodes\n",
    "    group by latitude, longitude, state\n",
    "    order by latitude, longitude, state\n",
    ")\n",
    "group by lat, long\n",
    "order by lat, long\n",
    "\"\"\")\n",
    "# aircode_table2.show(truncate=False)\n",
    "\n",
    "# join both tables to get the state with the most counts for each lat/long pairs\n",
    "aircode_table3 = aircode_table1.\\\n",
    "    join(aircode_table2, [aircode_table1.latitude == aircode_table2.lat, aircode_table1.longitude == aircode_table2.long, aircode_table1.num == aircode_table2.maxPerLatLong]).\\\n",
    "    drop('long', 'lat', 'num', 'maxPerLatLong').\\\n",
    "    withColumn(\"id_state_coord\", monotonically_increasing_id())\n",
    "\n",
    "aircode_table3.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5338113",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "We have 3 sets of data:\n",
    "1. immigration data\n",
    "2. temperature data\n",
    "3. airport codes\n",
    "\n",
    "Our immigration data is composed of arrival datetime fields (datetime column, month, dayofmonth, year) as well as some personal info (gender, birth year, state they're entering) as well as the airline people flew with. \n",
    "\n",
    "The temperature data is composed of temperatures linked to cities and countries. I have tried to join on latitude and longitude with 2 decimals but this is too fine-grained. \n",
    "A 2 decimals latitude/longitude corresponds to a real world distance of approx 1.11 km. There is no overlap between the places where temperatures are measured an airport locations.\n",
    "1 decimal corresponds to 11.1 km and zero to 111 km.\n",
    "\n",
    "The airport code data is composed of iso-region (i.e. US-PA) and coordinates data (longitude, latitude).\n",
    "\n",
    "In the end we'd like to relate the immigration data to temperatures.\n",
    "We cannot do this directly, so the goal is: \n",
    "1. read in immigration data, extract state.\n",
    "2. read in airport codes, extract latitude, longitude, round to zero decimals and extract state\n",
    "3. read in temperature data, extract latitude, longitude, round to zero decimals\n",
    "\n",
    "Using #3 we can create a helper table with columns: dayofmonth, month, lat, long, AvgTemp.\n",
    "The idea of this helper table is to determine an average temperature at a given latitude, longitude for a given day of month and month combination.\n",
    "\n",
    "The airport codes table can be linked to this helper table to determine an average temperature for each state for a given day of month and month combination.\n",
    "\n",
    "Finally, we can link immigration data to average temperatures via the day of month and month (of the arrival dates) and the state.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "##### Below contains the data dictionary as well!\n",
    "\n",
    "1. Read in immigration data, temperature data, airport codes data\n",
    "2. Select relevant columns\n",
    "    1. immigration data: i94port|biryear|gender |airline|i94visa|i94addr|arrdate_dt|depdate_dt|arrdate_dayofmonth|arrdate_month|arrdate_year\n",
    "    1. temperature data: dayofmonth|month|lat|long|AvgTemp           \n",
    "    1. airport codes data: lat|long|state\n",
    "3. create fact and dimension tables\n",
    "    1. dim_state_coord  \n",
    "        1. id_state_coord (integer, unique)  \n",
    "        1. state (string, state corresponding to rounded lat/long coordinate) \n",
    "        1. lat (latitude coordinate, rounded to zero decimals) \n",
    "        1. long (longitude coordinate, rounded to zero decimals) \n",
    "    1. dim_temp_coord  \n",
    "        1. id_temp_coord (integer, unique) \n",
    "        1. day_of_month (integer, day of month) \n",
    "        1. month (integer, month) \n",
    "        1. lat (latitude coordinate, rounded to zero decimals) \n",
    "        1. long (longitude coordinate, rounded to zero decimals) \n",
    "        1. AvgTemp (average temperature in degrees Celsius corresponding to latitude, longitude coordinate) \n",
    "    1. dim_time  \n",
    "        1. arr_dt (date, arrival datetime, unique) \n",
    "        1. day_of_month (integer, day of month) \n",
    "        1. month (integer, month) \n",
    "        1. year (integer, year) \n",
    "    1. dim_imm  \n",
    "        1. id_imm (integer, unique) \n",
    "        1. i94port (string, port of arrival)\n",
    "        1. biryear (integer, birth year of immigrant) \n",
    "        1. gender (string, gender of immigrant, is either M for male, F for female or unknown) \n",
    "        1. airline (string, airline people flew in with) \n",
    "        1. i94visa (integer, type of visa, filtered on 2 for Tourism) \n",
    "        1. i94addr (string, destination state)\n",
    "        1. arr_dt (date, arrival date) \n",
    "        1. dep_dt (date, departure date) \n",
    "    1. fact_imm  \n",
    "        1. id_imm (integer)  \n",
    "        1. id_state_coord (integer)  \n",
    "        1. id_temp_coord (integer)  \n",
    "        1. arr_dt (integer)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c62225",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fc318",
   "metadata": {},
   "source": [
    "# Create dimension tables\n",
    "\n",
    "Use .persist() on the smaller tables to optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52ff5c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e1b4bc1c764690ab7b2075314f35d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+--------------+\n",
      "|latitude|longitude|state|id_state_coord|\n",
      "+--------+---------+-----+--------------+\n",
      "|0       |0        |other|0             |\n",
      "|0       |1        |PA   |1             |\n",
      "|1       |0        |NC   |2             |\n",
      "|1       |1        |OK   |3             |\n",
      "|22      |160      |HI   |25769803776   |\n",
      "|24      |166      |HI   |25769803777   |\n",
      "|25      |80       |FL   |25769803778   |\n",
      "|25      |81       |FL   |25769803779   |\n",
      "|25      |82       |FL   |25769803780   |\n",
      "|26      |80       |FL   |25769803781   |\n",
      "+--------+---------+-----+--------------+\n",
      "\n",
      "+----------+-----+---+----+------------------+-------------+\n",
      "|dayofmonth|month|lat|long|AvgTemp           |id_temp_coord|\n",
      "+----------+-----+---+----+------------------+-------------+\n",
      "|1         |1    |27 |82  |18.587668103448276|17179869184  |\n",
      "|1         |2    |27 |82  |18.725623376623375|17179869185  |\n",
      "|1         |3    |27 |82  |19.821219827586205|17179869186  |\n",
      "|1         |4    |27 |82  |21.626489270386273|17179869187  |\n",
      "|1         |5    |27 |82  |24.153780172413803|17179869188  |\n",
      "|1         |6    |27 |82  |26.069188841201726|17179869189  |\n",
      "|1         |7    |27 |82  |26.762415254237297|25769803776  |\n",
      "|1         |8    |27 |82  |27.156209401709397|25769803777  |\n",
      "|1         |9    |27 |82  |26.93942672413793 |25769803778  |\n",
      "|1         |10   |27 |82  |24.84596506550217 |25769803779  |\n",
      "+----------+-----+---+----+------------------+-------------+\n",
      "\n",
      "+----------+----------+-----+----+\n",
      "|datetime  |dayofmonth|month|year|\n",
      "+----------+----------+-----+----+\n",
      "|2016-04-13|13        |4    |2016|\n",
      "|2016-04-30|30        |4    |2016|\n",
      "|2016-04-16|16        |4    |2016|\n",
      "|2016-04-08|8         |4    |2016|\n",
      "|2016-04-20|20        |4    |2016|\n",
      "|2016-05-22|22        |5    |2016|\n",
      "|2016-05-06|6         |5    |2016|\n",
      "|2016-05-18|18        |5    |2016|\n",
      "|2016-04-29|29        |4    |2016|\n",
      "|2016-04-06|6         |4    |2016|\n",
      "+----------+----------+-----+----+\n",
      "\n",
      "+-------+-------+-------+-------+-------+----------+----------+------------------+-------------+------------+-----+----------+\n",
      "|i94port|biryear|gender |airline|i94visa|arrdate_dt|depdate_dt|arrdate_dayofmonth|arrdate_month|arrdate_year|state|id_imm    |\n",
      "+-------+-------+-------+-------+-------+----------+----------+------------------+-------------+------------+-----+----------+\n",
      "|LVG    |1951.0 |M      |VS     |2.0    |2016-05-09|2016-05-22|9                 |5            |2016        |NV   |8589934592|\n",
      "|ORL    |1994.0 |unknown|BA     |2.0    |2016-05-09|2016-05-23|9                 |5            |2016        |FL   |8589934593|\n",
      "|MIA    |1994.0 |M      |BA     |2.0    |2016-05-09|2016-05-14|9                 |5            |2016        |FL   |8589934594|\n",
      "|SFR    |1986.0 |unknown|BA     |2.0    |2016-05-09|2016-05-31|9                 |5            |2016        |CA   |8589934595|\n",
      "|PIT    |1992.0 |M      |ZX     |2.0    |2016-05-09|2016-05-27|9                 |5            |2016        |PA   |8589934596|\n",
      "|NYC    |1964.0 |F      |LH     |2.0    |2016-05-09|2016-05-15|9                 |5            |2016        |NY   |8589934597|\n",
      "|LOS    |1963.0 |unknown|AB     |2.0    |2016-05-09|2016-05-23|9                 |5            |2016        |CA   |8589934598|\n",
      "|HHW    |1979.0 |F      |DL     |2.0    |2016-05-09|2016-05-12|9                 |5            |2016        |HI   |8589934599|\n",
      "|HHW    |1982.0 |F      |DL     |2.0    |2016-05-09|2016-05-12|9                 |5            |2016        |HI   |8589934600|\n",
      "|HHW    |1978.0 |F      |HA     |2.0    |2016-05-09|2016-05-13|9                 |5            |2016        |HI   |8589934601|\n",
      "+-------+-------+-------+-------+-------+----------+----------+------------------+-------------+------------+-----+----------+"
     ]
    }
   ],
   "source": [
    "dim_state_coord = aircode_table3.persist()\n",
    "dim_state_coord.limit(10).show(truncate=False)\n",
    "\n",
    "dim_temp_coord = temp_table.persist()\n",
    "dim_temp_coord.limit(10).show(truncate=False)\n",
    "\n",
    "imm.createOrReplaceTempView(\"immdata\")\n",
    "dim_time = spark.sql(\"\"\"\n",
    "select distinct arrdate_dt as datetime, arrdate_dayofmonth as dayofmonth, arrdate_month as month, arrdate_year as year\n",
    "from immdata\n",
    "\"\"\").persist()\n",
    "dim_time.limit(10).show(truncate=False)\n",
    "\n",
    "dim_imm = imm\n",
    "\n",
    "dim_imm.limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b479fd",
   "metadata": {},
   "source": [
    "# Create fact table\n",
    "\n",
    "The fact table should have 4 columns\n",
    "1. id_imm\n",
    "1. id_state_coord\n",
    "1. id_temp_coord\n",
    "1. arr_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad9d534b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4ca5027c3948bdb4354d8f6fb00b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5388905"
     ]
    }
   ],
   "source": [
    "print(dim_imm.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7eefbc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc4cc9c851d472cafaa8c301339ebaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17445547"
     ]
    }
   ],
   "source": [
    "fact_imm = dim_imm.\\\n",
    "    join(dim_time, [dim_imm.arrdate_dt == dim_time.datetime]).\\\n",
    "    join(dim_temp_coord, [dim_imm.arrdate_dayofmonth == dim_temp_coord.dayofmonth, dim_imm.arrdate_month == dim_temp_coord.month]).\\\n",
    "    join(dim_state_coord, [dim_temp_coord.lat == dim_state_coord.latitude, dim_temp_coord.long == dim_state_coord.longitude]).\\\n",
    "    select('id_imm', 'id_state_coord', 'id_temp_coord', 'datetime').\\\n",
    "    dropDuplicates()\n",
    "\n",
    "fact_imm.limit(10).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525a753",
   "metadata": {},
   "source": [
    "# Write the dimension/fact tables to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97c55422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3618ff0c01b34d3baced348f638810fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3_result_path = f\"s3a://jjudacitydatalake/capstone/processed/dim_state_coord\"\n",
    "\n",
    "dim_state_coord.\\\n",
    "    write.\\\n",
    "    format(\"parquet\").\\\n",
    "    mode(\"overwrite\").\\\n",
    "    save(s3_result_path)\n",
    "\n",
    "s3_result_path = f\"s3a://jjudacitydatalake/capstone/processed/dim_temp_coord\"\n",
    "\n",
    "dim_temp_coord.\\\n",
    "    write.\\\n",
    "    format(\"parquet\").\\\n",
    "    mode(\"overwrite\").\\\n",
    "    save(s3_result_path)\n",
    "\n",
    "\n",
    "s3_result_path = f\"s3a://jjudacitydatalake/capstone/processed/dim_time\"\n",
    "\n",
    "dim_time.\\\n",
    "    write.\\\n",
    "    format(\"parquet\").\\\n",
    "    mode(\"overwrite\").\\\n",
    "    save(s3_result_path)\n",
    "\n",
    "\n",
    "s3_result_path = f\"s3a://jjudacitydatalake/capstone/processed/dim_imm\"\n",
    "\n",
    "dim_imm.\\\n",
    "    write.\\\n",
    "    format(\"parquet\").\\\n",
    "    mode(\"overwrite\").\\\n",
    "    save(s3_result_path)\n",
    "\n",
    "\n",
    "s3_result_path = f\"s3a://jjudacitydatalake/capstone/processed/fact_imm\"\n",
    "\n",
    "fact_imm.\\\n",
    "    write.\\\n",
    "    format(\"parquet\").\\\n",
    "    mode(\"overwrite\").\\\n",
    "    save(s3_result_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee5d85b",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "495d31b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf11aba543ce45e293b8f9b0cb978384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.setJobGroup(\"DataQuality\", \"Counting number of records in tables\")\n",
    "\n",
    "def recordCount(table_name):\n",
    "    \"\"\"\n",
    "    Given a dataframe table_name, return the number of records in it\n",
    "    \"\"\"\n",
    "    return table_name.count()\n",
    "\n",
    "def checkNumberOfRows(actual_count, expected_count):\n",
    "    \"\"\"\n",
    "    Given a number actual_count, compare to expected_count and raise ValueError exception if it differs.\n",
    "    \"\"\"\n",
    "    if actual_count != expected_count:\n",
    "        raise ValueError(f\"The number of records found is {actual_count}, differing from expected value {expected_count}\")\n",
    "\n",
    "expectedRowCount = {dim_state_coord : 1200, dim_temp_coord : 1164, dim_time : 61, dim_imm : 5388905, fact_imm : 17445547}\n",
    "                         \n",
    "for obj in [dim_state_coord, dim_temp_coord, dim_time, dim_imm, fact_imm]:\n",
    "    numRows = recordCount(obj)\n",
    "    \n",
    "    checkNumberOfRows(numRows, expectedRowCount[obj])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3e5b6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3381d5721a43b4a56102e51f22a3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imm.createOrReplaceTempView('immdata')\n",
    "# sc.setJobGroup(\"DataQuality\", \"Selecting distinct states\")\n",
    "# result = spark.sql(\"\"\"\n",
    "# select distinct state\n",
    "# from immdata\n",
    "# order by state asc\n",
    "# \"\"\")\n",
    "# result.show(result.count(), truncate=False)\n",
    "\n",
    "sc.setJobGroup(\"DataQuality\", \"Counting total number of distinct states\")\n",
    "numDistinctStates = spark.sql(\"\"\"\n",
    "select count(distinct state) \n",
    "from immdata\n",
    "\"\"\")\n",
    "\n",
    "checkNumberOfRows(numDistinctStates.collect()[0]['count(DISTINCT state)'], len(valid_us_states) + 1)\n",
    "# result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3aa7e",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "In this project we choose the combination of AWS S3 for fast & efficient storage with an AWS EMR cluster for efficient processing on large datasets.\n",
    "S3 is used as well to store the resulting analysis.\n",
    "\n",
    "As we have relatively static data (immigration data grows relatively slowly with one day at a time) this analysis was set up as a run-once thing. Meaning, we put the data somewhere, run a one-time analysis and proceed wit hthe results.\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "Temperature data currently is averaged out over many years so there's no need to update it often unless weather conditions across the US change a lot. \n",
    "\n",
    "Immigration data will have grown by a year after one year of waiting so a yearly frequency might be in order.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.<br>\n",
    "If the data was increased by 100x then I'd probably look into partitioning the data into smaller partitions. Furthermore, I would increase the number of nodes on the EMR cluster for faster processing. I'd also consider moving the data from S3 to Hadoop FS to optimize the I/O across the network.\n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.<br>\n",
    "I'd create an Airflow workflow to easily schedule this task. Furthermore I'd process the history once and rewrite the code such that only the daily data added would be processed.\n",
    " \n",
    " * The database needed to be accessed by 100+ people.<br>\n",
    "If the database needed to be accessed by 100+ people I'd run the analysis once and cache the results somewhere and serve the cache. As the setup currently is now, there is no user input that could change the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a3518",
   "metadata": {},
   "source": [
    "# Analysis example\n",
    "\n",
    "Find out what state has the most tourists per month and what is the temperature like at the destination?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91f6d92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af09dc02efda494cbbc905ace6fd6901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+------------------+-----------+\n",
      "|arrdate_month|state|avg(AvgTemp)      |tourist_num|\n",
      "+-------------+-----+------------------+-----------+\n",
      "|4            |FL   |15.319045976272758|678321     |\n",
      "|4            |NY   |15.399508138264684|564840     |\n",
      "|4            |CA   |15.399421013491569|434376     |\n",
      "|5            |FL   |17.65150757190719 |418473     |\n",
      "|5            |NY   |17.402843662849904|335907     |\n",
      "|5            |CA   |17.546777904672375|284391     |\n",
      "|4            |other|15.152546680569664|269649     |\n",
      "|5            |other|18.016987491220323|193266     |\n",
      "|4            |HI   |15.469318783133671|175311     |\n",
      "|5            |HI   |17.754536895845458|141372     |\n",
      "+-------------+-----+------------------+-----------+"
     ]
    }
   ],
   "source": [
    "sc.setJobGroup(\"Analysis\", \"Counting number of tourists per month and the average temperature therein\")\n",
    "\n",
    "fact_imm.createOrReplaceTempView(\"fact_imm\")\n",
    "dim_temp_coord.createOrReplaceTempView(\"temp\")\n",
    "dim_imm.createOrReplaceTempView(\"imm\")\n",
    "# select arrdate_month, state, imm.id_imm, fact_imm.id_imm, temp.id_temp_coord, fact_imm.id_temp_coord, temp.AvgTemp\n",
    "analysis1 = spark.sql(\"\"\"\n",
    "select arrdate_month, state, avg(temp.AvgTemp), count(*) as tourist_num\n",
    "from imm\n",
    "join fact_imm on (imm.id_imm = fact_imm.id_imm)\n",
    "join temp on (temp.id_temp_coord = fact_imm.id_temp_coord)\n",
    "group by arrdate_month, state\n",
    "order by tourist_num desc\n",
    "\"\"\")\n",
    "analysis1.limit(10).show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
