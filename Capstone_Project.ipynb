{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Our objective is to gain insight in tourism by finding out how many people are traveling to destinations (states), what the weather conditions (average temperature) are like at that moment in time so tourism operators can adjust their offerings accordingly. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# install pandas package, required for .toPandas() functionality\r\n",
    "sc.install_pypi_package(\"pandas==0.25.1\") "
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376db495e94946f2a8947282237aa70a",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1627990089475_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-37-184.us-west-2.compute.internal:20888/proxy/application_1627990089475_0002/\" class=\"emr-proxy-link\" emr-resource=\"j-FSQHRDBCDC5G\n",
       "\" application-id=\"application_1627990089475_0002\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-34-71.us-west-2.compute.internal:8042/node/containerlogs/container_1627990089475_0002_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pandas==0.25.1\n",
      "  Using cached https://files.pythonhosted.org/packages/7e/ab/ea76361f9d3e732e114adcd801d2820d5319c23d0ac5482fa3b412db217e/pandas-0.25.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==0.25.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==0.25.1)\n",
      "Collecting python-dateutil>=2.6.1 (from pandas==0.25.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/36/7a/87837f39d0296e723bb9b62bbb257d0355c7f6128853c78955f57342a56d/python_dateutil-2.8.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==0.25.1)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-0.25.1 python-dateutil-2.8.2"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pyspark.sql.functions import udf, to_date, col, month, year, dayofmonth, split, format_string, abs, isnan, when, count, substring, length, regexp_extract, monotonically_increasing_id\r\n",
    "from datetime import datetime\r\n",
    "from datetime import timedelta\r\n",
    "\r\n",
    "import pyspark.sql.types as t\r\n",
    "# import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import logging\r\n",
    "import configparser"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3dfede56b54080b8a5c7a3e7e025d7",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# settings, specify the s3 bucket name on which to read data from and store data on\r\n",
    "# config = configparser.ConfigParser()\r\n",
    "# config.read('settings.cfg')\r\n",
    "\r\n",
    "# ensure that the environment variables are set before the spark session is started, otherwise s3 cannot be accessed\r\n",
    "# os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\r\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\r\n",
    "# s3_bucket=config['AWS']['AWS_S3_BUCKET_LOC']\r\n",
    "s3_bucket='jjudacitydatalake'"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e01465f25394e768fa8a7a30d8dd9c3",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "We want to gain insight in tourism in the United States. Questions like, how many people are traveling to destinations and what are the weather conditions like at the arrival location. The goal is for tourism operators to gain insight in why people travel there so they can adjust their tourism offerings. \n",
    "\n",
    "We use the immigration dataset provided by Udacity (originally from the US National Tourism and Trade Office) which can be found here: https://travel.trade.gov/research/reports/i94/historical/2016.html. \n",
    "\n",
    "Our goal is to run a one-time analysis on the full (i.e. the whole year) dataset. We want to match the immigration data to historical temperatures.\n",
    "One particular example is to count how many people are arriving in a certain state per time period (year, month) and what the expected historical average temperature is like at that period in time.\n",
    "\n",
    "The final data model provides information on immigration data as well as temperature data. Each record of the immigration data contains information about the specifics of an arrival into the US (such as airline, port of arrival, state) as well as some specifics on the person arriving (such as gender, birth year). \n",
    "Temperature data is originally provided on a City/Country basis which makes it hard to join with the immigration data. The geographic information contained in the immigration data is the state so our ETL uses the coordinate data to link City/Country information to State (with the intermediate use of the airport code table).\n",
    "An example analysis of how a user may use the data model to gain insight is given below in the notebook. See also the table design below in the notebook for information on how users might link the available information together.\n",
    "\n",
    "We'll store the immigration data and the temperature data on S3 with a suitable partitioning for efficient processing. The processing itself will be done via Apache Spark on an EMR cluster. Results will be written back to S3.\n",
    "\n",
    "Users will be able to read in the processed parquet files on S3 from the /capstone/processed/ folders. With a tool like Power BI, they will be able to read in the star schema and perform their analysis.\n",
    "One example of an analysis is shown in this notebook (see below).\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### Immigration data\n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "We use the immigration dataset provided by Udacity (originally from the US National Tourism and Trade Office) which can be found here: https://travel.trade.gov/research/reports/i94/historical/2016.html. The US National Tourism and Trade Office provides this dataset to the public for third parties to gain useful insights. Each entry in the dataset is an arrival into the USA with additional data provided (such as gender, date of arrival, airline, purposes of visit (we filter on tourism/pleasure).\n",
    "\n",
    "The Immigration data has been provided by a separate disk on the Udacity workspace. I have read this data in with pandas and re-written it without modifications as parquet files in the i94parquet folder on my S3 folder.\n",
    "\n",
    "##### Temperature data\n",
    "This dataset was provided by Udacity and is originally sourced from Kaggle. More information can be found here: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data.\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def readMultipleParquet(listPaths):\r\n",
    "    \"\"\"\r\n",
    "    Given a list of paths, return Spark Dataframe for processing\r\n",
    "    \"\"\"\r\n",
    "    sc.setJobGroup(\"Read\", \"Reading multiple parquet files\")\r\n",
    "    return spark.read.parquet(*listPaths)\r\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984f9f3f686847c193241062b5ca80a8",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def sasDateToDatetime(sasdate):\r\n",
    "    \"\"\"\r\n",
    "    Given a spark column which specifies the number of days since 1960, return the datetime object\r\n",
    "    \"\"\"\r\n",
    "    return None if sasdate == None else datetime.strptime('1960-01-01', \"%Y-%m-%d\") + timedelta(sasdate)\r\n",
    "\r\n",
    "valid_us_states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \r\n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \r\n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \r\n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \r\n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\r\n",
    "\r\n",
    "sasdate_udf = udf(sasDateToDatetime, t.DateType())\r\n",
    "\r\n",
    "def read_immigration_staging(listPaths):\r\n",
    "    \"\"\"\r\n",
    "    Given a list of strings representing paths to sas data files stored in parquet format, read and transform the immmigration staging data and return the DataFrame.\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    sc.setJobGroup(\"Read\", \"Read raw immigration staging data\")\r\n",
    "\r\n",
    "    raw_data = readMultipleParquet(listPaths)\r\n",
    "\r\n",
    "    print(f\"Number of raw rows read: {raw_data.count()}\")\r\n",
    "    \r\n",
    "    sc.setJobGroup(\"Read\", \"Read and transform immigration staging data\")\r\n",
    "    \r\n",
    "    final_data = raw_data.\\\r\n",
    "        withColumn('arrdate_dt', sasdate_udf('arrdate')).\\\r\n",
    "        withColumn('depdate_dt', sasdate_udf('depdate')).\\\r\n",
    "        withColumn('arrdate_dayofmonth', dayofmonth(col('arrdate_dt'))).\\\r\n",
    "        withColumn('arrdate_month', month(col('arrdate_dt'))).\\\r\n",
    "        withColumn('arrdate_year', year(col('arrdate_dt'))).\\\r\n",
    "        withColumn('state', when(~col('i94addr').isin(valid_us_states), 'other').otherwise(col('i94addr'))).\\\r\n",
    "        fillna('other', subset='state').\\\r\n",
    "        fillna('unknown', subset='gender').\\\r\n",
    "        dropDuplicates().\\\r\n",
    "        select('i94port', 'biryear', 'gender', 'airline', 'i94visa', 'arrdate_dt', 'depdate_dt', 'arrdate_dayofmonth', 'arrdate_month', 'arrdate_year', 'state').\\\r\n",
    "        filter(col('i94visa') == 2).\\\r\n",
    "        withColumn('id_imm', monotonically_increasing_id())\r\n",
    "\r\n",
    "    print(f\"Number of rows in final selected dataset: {final_data.count()}\")\r\n",
    "    \r\n",
    "    return raw_data, final_data\r\n",
    "\r\n",
    "immigration_raw_data, immigration_final_data = read_immigration_staging([f's3://{s3_bucket}/capstone/staging/i94_parquet/i94_apr16_sub.sas7bdat', f's3://{s3_bucket}/capstone/staging/i94_parquet/i94_may16_sub.sas7bdat'])\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7007aed18347c89b170afd3f45bf2b",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of raw rows read: 6540562\n",
      "Number of rows in final selected dataset: 5388905"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examine the raw immigration data. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "immigration_raw_data.limit(10).show(truncate=False)\r\n",
    "\r\n",
    "immigration_raw_data.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686735cb0daf4925916f75c9fcec6192",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|cicid    |i94yr |i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto |gender|insnum|airline|admnum         |fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|1689141.0|2016.0|5.0   |135.0 |135.0 |LVG    |20583.0|1.0    |NE     |20590.0|27.0  |2.0    |1.0  |20160509|null    |null |G      |O      |null   |M      |1989.0 |08062016|M     |null  |VS     |6.0079838633E10|00043|WT      |\n",
      "|1689142.0|2016.0|5.0   |135.0 |135.0 |LVG    |20583.0|1.0    |NE     |20590.0|26.0  |2.0    |1.0  |20160509|null    |null |G      |O      |null   |M      |1990.0 |08062016|F     |null  |VS     |6.0082092733E10|00043|WT      |\n",
      "|1689143.0|2016.0|5.0   |135.0 |135.0 |LVG    |20583.0|1.0    |NE     |20590.0|26.0  |2.0    |1.0  |20160509|null    |null |G      |O      |null   |M      |1990.0 |08062016|M     |null  |VS     |6.0080009033E10|00043|WT      |\n",
      "|1689144.0|2016.0|5.0   |135.0 |135.0 |LVG    |20583.0|1.0    |NE     |20590.0|26.0  |2.0    |1.0  |20160509|null    |null |G      |O      |null   |M      |1990.0 |08062016|M     |null  |VS     |6.0082576733E10|00043|WT      |\n",
      "|1689145.0|2016.0|5.0   |135.0 |135.0 |LVG    |20583.0|1.0    |NE     |20590.0|25.0  |2.0    |1.0  |20160509|null    |null |G      |O      |null   |M      |1991.0 |08062016|F     |null  |VS     |6.0081937833E10|00043|WT      |\n",
      "|1689146.0|2016.0|5.0   |135.0 |135.0 |LVG    |20583.0|1.0    |NE     |20590.0|25.0  |2.0    |1.0  |20160509|null    |null |G      |O      |null   |M      |1991.0 |08062016|M     |null  |VS     |6.0082086233E10|00043|WT      |\n",
      "|1689147.0|2016.0|5.0   |135.0 |135.0 |LVG    |20583.0|1.0    |NE     |20590.0|66.0  |2.0    |1.0  |20160509|null    |null |G      |O      |null   |M      |1950.0 |08062016|M     |null  |VS     |6.0082330033E10|00043|WT      |\n",
      "|1689148.0|2016.0|5.0   |135.0 |135.0 |LVG    |20583.0|1.0    |NE     |20590.0|60.0  |2.0    |1.0  |20160509|null    |null |G      |O      |null   |M      |1956.0 |08062016|F     |null  |VS     |6.0082228333E10|00043|WT      |\n",
      "|1689149.0|2016.0|5.0   |135.0 |135.0 |LVG    |20583.0|1.0    |NE     |20592.0|75.0  |2.0    |1.0  |20160509|null    |null |G      |O      |null   |M      |1941.0 |08062016|M     |null  |VS     |6.0081989033E10|00043|WT      |\n",
      "|1689150.0|2016.0|5.0   |135.0 |135.0 |LVG    |20583.0|1.0    |NE     |20592.0|73.0  |2.0    |1.0  |20160509|null    |null |G      |O      |null   |M      |1943.0 |08062016|F     |null  |VS     |6.0081878133E10|00043|WT      |\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset enhancement\n",
    "\n",
    "### This is part of Step 2: Explore and Assess the Data\n",
    "Identify data quality issues, like missing values, duplicate data, etc. & Document steps necessary to clean the data.\n",
    "These steps are performed in method read_immigration_staging.\n",
    "\n",
    "## Tourism filtering\n",
    "We are interested in tourism data so from the data description we find we have to filter on i94visa = 2 (Pleasure).\n",
    "\n",
    "<code>\n",
    "/* I94VISA - Visa codes collapsed into three categories:\n",
    "   1 = Business\n",
    "   2 = Pleasure\n",
    "   3 = Student\n",
    "*/\n",
    "</code>\n",
    "\n",
    "## Adding useful date fields\n",
    "The arrdate and depdate fields are double. They represent the number of days since 1-1-1960.\n",
    "We add columns arrdate_dt, depdate_dt (to parse them as proper datetime objects) as well as day of month, month and year columns which can be extracted from arrdate_dt.\n",
    "\n",
    "## Fixing US states\n",
    "The US contains 50 states. The field i94addr can contain invalid data (i.e. entries outside the applicable list of 50 states). If this happens we replace the value with 'other'.\n",
    "We also replace values null with other.\n",
    "\n",
    "## Dropping duplicates\n",
    "We drop duplicate rows in the immigration dataset. Each row ought to be unique.\n",
    "\n",
    "## Gender\n",
    "Sometimes gender is null. As there's no way of enriching the data we substitute the null value with 'unknown'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examine result after data enhancement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "immigration_final_data.limit(10).toPandas()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512334026faa44229dac149c366acbbc",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  i94port  biryear   gender airline  ...  arrdate_month arrdate_year state  id_imm\n",
      "0     NYC   1975.0        M      AM  ...              5         2016    NY       0\n",
      "1     NYC   1950.0        M      FI  ...              5         2016    NY       1\n",
      "2     NYC   1984.0        M      VS  ...              5         2016    NY       2\n",
      "3     NYC   1949.0        M      BA  ...              5         2016    NY       3\n",
      "4     ORL   1969.0        F      VS  ...              5         2016    FL       4\n",
      "5     ORL   1993.0  unknown      BA  ...              5         2016    FL       5\n",
      "6     MIA   1987.0        M      BA  ...              5         2016    FL       6\n",
      "7     MIA   1947.0        F      BA  ...              5         2016    FL       7\n",
      "8     TAM   1975.0  unknown      BA  ...              5         2016    FL       8\n",
      "9     MIA   2001.0        F      AB  ...              5         2016    FL       9\n",
      "\n",
      "[10 rows x 12 columns]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "immigration_final_data.limit(10).show(truncate=False)\n",
    "\n",
    "immigration_final_data.printSchema()\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e817957415e4b2bb13a9f89cf931c8d",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-------+-------+-------+-------+----------+----------+------------------+-------------+------------+-----+----------+\n",
      "|i94port|biryear|gender |airline|i94visa|arrdate_dt|depdate_dt|arrdate_dayofmonth|arrdate_month|arrdate_year|state|id_imm    |\n",
      "+-------+-------+-------+-------+-------+----------+----------+------------------+-------------+------------+-----+----------+\n",
      "|LVG    |1951.0 |M      |VS     |2.0    |2016-05-09|2016-05-22|9                 |5            |2016        |NV   |8589934592|\n",
      "|ORL    |1994.0 |unknown|BA     |2.0    |2016-05-09|2016-05-23|9                 |5            |2016        |FL   |8589934593|\n",
      "|MIA    |1994.0 |M      |BA     |2.0    |2016-05-09|2016-05-14|9                 |5            |2016        |FL   |8589934594|\n",
      "|SFR    |1986.0 |unknown|BA     |2.0    |2016-05-09|2016-05-31|9                 |5            |2016        |CA   |8589934595|\n",
      "|PIT    |1992.0 |M      |ZX     |2.0    |2016-05-09|2016-05-27|9                 |5            |2016        |PA   |8589934596|\n",
      "|NYC    |1964.0 |F      |LH     |2.0    |2016-05-09|2016-05-15|9                 |5            |2016        |NY   |8589934597|\n",
      "|LOS    |1963.0 |unknown|AB     |2.0    |2016-05-09|2016-05-23|9                 |5            |2016        |CA   |8589934598|\n",
      "|HHW    |1979.0 |F      |DL     |2.0    |2016-05-09|2016-05-12|9                 |5            |2016        |HI   |8589934599|\n",
      "|HHW    |1982.0 |F      |DL     |2.0    |2016-05-09|2016-05-12|9                 |5            |2016        |HI   |8589934600|\n",
      "|HHW    |1978.0 |F      |HA     |2.0    |2016-05-09|2016-05-13|9                 |5            |2016        |HI   |8589934601|\n",
      "+-------+-------+-------+-------+-------+----------+----------+------------------+-------------+------------+-----+----------+\n",
      "\n",
      "root\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- gender: string (nullable = false)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- arrdate_dt: date (nullable = true)\n",
      " |-- depdate_dt: date (nullable = true)\n",
      " |-- arrdate_dayofmonth: integer (nullable = true)\n",
      " |-- arrdate_month: integer (nullable = true)\n",
      " |-- arrdate_year: integer (nullable = true)\n",
      " |-- state: string (nullable = false)\n",
      " |-- id_imm: long (nullable = false)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Database normalization\n",
    "Above we have a selection of useful columns. We have picked these as they contain useful information about the people arriving as well as the immigration details (port of arrival i94port, visa type (i94visa) and airline.\n",
    "\n",
    "In order to properly store this information we apply the database normalization technique covered earlier in order to come up with a robust database design. We do this to minimize duplicate data, to minimize data modification issues and to simplify queries.\n",
    "\n",
    "When we look at the above table we see a lot of information: port of arrival, date of birth of the person entering, gender of the person entering, airline the person entered with, visa type (Tourism in this case), arrival datetime, departure datetime, day of month, month and year of arrival date, destination state and unique id id_imm.\n",
    "There are several topics covered in this table: airport, person entering on date, airlines and state.\n",
    "\n",
    "We will later on split these tables in a more proper format.\n",
    "\n",
    "<img src=\"https://i.imgur.com/jHBgMur.png\">\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read in immigration data\n",
    "\n",
    "Explore data skewness, find partition/clustering keys in order to parallelize processing\n",
    "\n",
    "I have a gut feeling that the number of people arriving each month and day of month ought to be relatively constant. Let's validate this assumption:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "immigration_final_data.createOrReplaceTempView(\"immdata\")\n",
    "dataSkewQuery = spark.sql(\"\"\"\n",
    "select arrdate_month as month, arrdate_dayofmonth as day, count(*) as immnum\n",
    "from immdata\n",
    "group by month, day\n",
    "order by month asc, day asc\n",
    "\"\"\")\n",
    "dataSkewQuery.toPandas()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6848a57d9d384415908b9990b6d2fa46",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    month  day  immnum\n",
      "0       4    1   95254\n",
      "1       4    2   83883\n",
      "2       4    3   66613\n",
      "3       4    4   69904\n",
      "4       4    5   74432\n",
      "..    ...  ...     ...\n",
      "56      5   27  107975\n",
      "57      5   28  104771\n",
      "58      5   29   75577\n",
      "59      5   30   64529\n",
      "60      5   31   69553\n",
      "\n",
      "[61 rows x 3 columns]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examine seasonality in the data, how many visitors visit the US each month?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "immigration_final_data.createOrReplaceTempView(\"immdata\")\n",
    "seasonality_table = spark.sql(\"\"\"\n",
    "select arrdate_month as month, count(*) as immnum\n",
    "from immdata\n",
    "group by month\n",
    "order by month asc\n",
    "\"\"\")\n",
    "seasonality_table.toPandas()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2d2acbc65644a4b7aff32bbf13ce9c",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   month   immnum\n",
      "0      4  2530868\n",
      "1      5  2858037"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read in temperature data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def read_temperature_staging(listPaths):\n",
    "    \"\"\"\n",
    "    Given a list of paths to csv files, read in the temperature data, run transform and return dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    sc.setJobGroup(\"Read\", \"Read raw temperature staging data\")\n",
    "    raw_data = spark.read.option(\"header\", \"true\").csv(*listPaths)\n",
    "\n",
    "    sc.setJobGroup(\"Read\", \"Read and transform temperature staging data\")\n",
    "    final_data = raw_data.\\\n",
    "    filter(col('Country') == 'United States').\\\n",
    "    select(to_date(col(\"dt\"),\"yyyy-MM-dd\").alias(\"dt\"), 'AverageTemperature', 'City', 'Country', 'Latitude', 'Longitude').\\\n",
    "    withColumn('dayofmonth', dayofmonth(col('dt'))).\\\n",
    "    withColumn('month', month(col('dt'))).\\\n",
    "    withColumn('year', year(col('dt'))).\\\n",
    "    withColumn(\"latitude_rounded\", format_string(\"%.0f\", regexp_extract(col('Latitude'), '\\d+.\\d+', 0).cast(t.DoubleType()))).\\\n",
    "    withColumn(\"longitude_rounded\", format_string(\"%.0f\", regexp_extract(col('Longitude'), '\\d+.\\d+', 0).cast(t.DoubleType()))).\\\n",
    "    dropna()\n",
    "    \n",
    "    return raw_data, final_data\n",
    "#     raw_data.limit(10).show(truncate=False)\n",
    "\n",
    "temperature_raw_data, temperature_final_data = read_temperature_staging([f's3://{s3_bucket}/capstone/staging/temperature_data/GlobalLandTemperaturesByCity.csv'])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b4321c656042429d294e0b3d255748",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\n",
    "temperature_final_data.createOrReplaceTempView(\"tempdata_coord\")\n",
    "temp_table = spark.sql(\"\"\"\n",
    "select dayofmonth, month, latitude_rounded as lat, longitude_rounded as long, avg(AverageTemperature) as AvgTemp\n",
    "from tempdata_coord\n",
    "group by lat, long, month, dayofmonth\n",
    "order by lat asc, long asc, month asc, dayofmonth asc\n",
    "\"\"\")\n",
    "\n",
    "# temp_table = temp_table.withColumn(\"id_temp_coord\", monotonically_increasing_id())\n",
    "temp_table.show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5def54198f764306ac816aacec825659",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----+---+----+------------------+\n",
      "|dayofmonth|month|lat|long|AvgTemp           |\n",
      "+----------+-----+---+----+------------------+\n",
      "|1         |1    |27 |81  |17.598033333333337|\n",
      "|1         |2    |27 |81  |18.65918828451882 |\n",
      "|1         |3    |27 |81  |20.510491666666674|\n",
      "|1         |4    |27 |81  |22.596529166666656|\n",
      "|1         |5    |27 |81  |25.009309623430973|\n",
      "|1         |6    |27 |81  |27.00768049792531 |\n",
      "|1         |7    |27 |81  |27.85293775933608 |\n",
      "|1         |8    |27 |81  |27.92431535269709 |\n",
      "|1         |9    |27 |81  |26.861252100840332|\n",
      "|1         |10   |27 |81  |23.967126582278475|\n",
      "|1         |11   |27 |81  |20.727659663865527|\n",
      "|1         |12   |27 |81  |18.027008403361332|\n",
      "|1         |1    |27 |82  |18.587668103448276|\n",
      "|1         |2    |27 |82  |18.725623376623375|\n",
      "|1         |3    |27 |82  |19.821219827586205|\n",
      "|1         |4    |27 |82  |21.626489270386273|\n",
      "|1         |5    |27 |82  |24.153780172413803|\n",
      "|1         |6    |27 |82  |26.069188841201726|\n",
      "|1         |7    |27 |82  |26.762415254237297|\n",
      "|1         |8    |27 |82  |27.156209401709397|\n",
      "+----------+-----+---+----+------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read in airport code data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mapping coordinates to states\n",
    "We introduce a bit of noise by rounding the lat/long coordinates to 0 decimals. This means that each lat, long pair can have multiple states, which is undesirable.\n",
    "\n",
    "Below we count the number of states contained within each lat/long pair and select the maximum as being the most representative state for each lat/long coordinate."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def read_airport_codes_staging(listPaths):\n",
    "    \"\"\"\n",
    "    Given a list of strings to airport code csv files, return the raw and final dataframes of airport codes.\n",
    "    \"\"\"\n",
    "    \n",
    "    sc.setJobGroup(\"Read\", \"Read raw airport code staging data\")\n",
    "    raw_data = spark.read.option(\"header\", \"true\").csv(*listPaths)\n",
    "    \n",
    "    # coordinates are specified in [longitude, latitude]\n",
    "    coordinates_split = split(raw_data['coordinates'], ',')\n",
    "    region_split = split(raw_data['iso_region'], '-')\n",
    "\n",
    "    sc.setJobGroup(\"Read\", \"Read and transform airport code staging data\")\n",
    "    final_data = raw_data.\\\n",
    "                        filter(col('iso_country') == 'US').\\\n",
    "                        withColumn(\"latitude\", format_string(\"%.0f\", abs(coordinates_split.getItem(1).cast(t.DoubleType())))).\\\n",
    "                        withColumn(\"longitude\", format_string(\"%.0f\", abs(coordinates_split.getItem(0).cast(t.DoubleType())))).\\\n",
    "                        withColumn(\"state\", region_split.getItem(1)).\\\n",
    "                        withColumn('state', when(~col('state').isin(valid_us_states), 'other').otherwise(col('state'))).\\\n",
    "                        fillna('other', subset='state')\n",
    "    \n",
    "    return raw_data, final_data\n",
    "\n",
    "airport_codes_raw_data, airport_codes_final_data = read_airport_codes_staging([f's3://{s3_bucket}/capstone/staging/airportcodes_data/airport-codes_csv.csv'])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0764f0632c2a4cff962811c79260fbd5",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def create_temperature_table():\n",
    "    \"\"\"\n",
    "    Given two dataframes, airport_codes_final_data and temperature_final_data, join these on coordinates and return a dataframe\n",
    "    with columns day of month, month, state and the average temperature applicable\n",
    "    \"\"\"\n",
    "    sc.setJobGroup(\"Transform\", \"Create temperature helper table\")\n",
    "    \n",
    "    temperature_final_data.createOrReplaceTempView(\"tempdata_coord\")\n",
    "    temp_table = spark.sql(\"\"\"\n",
    "    select dayofmonth, month, latitude_rounded as lat, longitude_rounded as long, avg(AverageTemperature) as AvgTemp\n",
    "    from tempdata_coord\n",
    "    group by lat, long, month, dayofmonth\n",
    "    order by lat asc, long asc, month asc, dayofmonth asc\n",
    "    \"\"\")\n",
    "\n",
    "    airport_codes_final_data.createOrReplaceTempView(\"aircodes\")\n",
    "    # count the number of states for each lat/long pair\n",
    "    aircode_table1 = spark.sql(\"\"\"\n",
    "    select latitude, longitude, state, count(state) as num\n",
    "    from aircodes\n",
    "    group by latitude, longitude, state\n",
    "    order by latitude, longitude, state\n",
    "    \"\"\")\n",
    "\n",
    "    airport_codes_final_data.createOrReplaceTempView(\"aircodes\")\n",
    "    # determine the maximum count per lat/long pair\n",
    "    aircode_table2 = spark.sql(\"\"\"\n",
    "    select latitude as lat, longitude as long, max(num) as maxPerLatLong from (\n",
    "        select latitude, longitude, state, count(state) as num\n",
    "        from aircodes\n",
    "        group by latitude, longitude, state\n",
    "        order by latitude, longitude, state\n",
    "    )\n",
    "    group by lat, long\n",
    "    order by lat, long\n",
    "    \"\"\")\n",
    "\n",
    "    # join both tables to get the state with the most counts for each lat/long pairs\n",
    "    aircode_table3 = aircode_table1.\\\n",
    "        join(aircode_table2, [aircode_table1.latitude == aircode_table2.lat, aircode_table1.longitude == aircode_table2.long, aircode_table1.num == aircode_table2.maxPerLatLong]).\\\n",
    "        drop('long', 'lat', 'num', 'maxPerLatLong')\n",
    "\n",
    "    # finally, join both together on coordinates\n",
    "    state_temp = temp_table.join(aircode_table3, [temp_table.lat == aircode_table3.latitude, temp_table.long == aircode_table3.longitude])\n",
    "\n",
    "    state_temp.createOrReplaceTempView(\"state_temp\")\n",
    "    state_temp2 = spark.sql(\"\"\"\n",
    "    select dayofmonth, month, state, avg(AvgTemp) as avg_temp\n",
    "    from state_temp\n",
    "    group by dayofmonth, month, state\n",
    "    order by dayofmonth, month, state\n",
    "    \"\"\")\n",
    "    \n",
    "    state_temp2 = state_temp2.withColumn(\"id_temp\", monotonically_increasing_id())\n",
    "    \n",
    "    return state_temp2\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a78846235f94cac9c0fdf2d41ecade6",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "state_temp = create_temperature_table()\n",
    "\n",
    "state_temp.show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa2b7b091c14e7d85584ab1a82843b8",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----+-----+--------------------+----------+\n",
      "|dayofmonth|month|state|avg_temp            |id_temp   |\n",
      "+----------+-----+-----+--------------------+----------+\n",
      "|1         |1    |AK   |-15.762100000000002 |0         |\n",
      "|1         |1    |AL   |7.015045627376424   |1         |\n",
      "|1         |1    |AR   |4.856640816326534   |2         |\n",
      "|1         |1    |AZ   |9.667974719101123   |3         |\n",
      "|1         |1    |CA   |7.45849751585624    |4         |\n",
      "|1         |1    |CO   |-4.064113748795792  |5         |\n",
      "|1         |1    |CT   |-0.9482965779467679 |6         |\n",
      "|1         |1    |FL   |15.287752127682106  |7         |\n",
      "|1         |1    |GA   |7.619192648922688   |8         |\n",
      "|1         |1    |IA   |-7.177857954545457  |9         |\n",
      "|1         |1    |IL   |-3.804348052771056  |10        |\n",
      "|1         |1    |IN   |-2.4980444139589815 |11        |\n",
      "|1         |1    |KS   |-1.9120205612310877 |12        |\n",
      "|1         |1    |KY   |0.8350912547528511  |13        |\n",
      "|1         |1    |LA   |10.256176014868858  |14        |\n",
      "|1         |1    |MD   |-0.35399619771863106|15        |\n",
      "|1         |1    |MI   |-5.180329545454547  |16        |\n",
      "|1         |1    |MN   |-12.000909599039176 |8589934592|\n",
      "|1         |1    |MO   |-1.6898081632653068 |8589934593|\n",
      "|1         |1    |MS   |8.11926141078838    |8589934594|\n",
      "+----------+-----+-----+--------------------+----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "We have 3 sets of data:\n",
    "1. immigration data\n",
    "2. temperature data\n",
    "3. airport codes\n",
    "\n",
    "Our immigration data is composed of arrival datetime fields (datetime column, month, dayofmonth, year) as well as some personal info (gender, birth year, state they're entering) as well as the airline people flew with. \n",
    "\n",
    "The temperature data is composed of temperatures linked to cities and countries. I have tried to join on latitude and longitude with 2 decimals but this is too fine-grained. \n",
    "A 2 decimals latitude/longitude corresponds to a real world distance of approx 1.11 km. There is no overlap between the places where temperatures are measured an airport locations.\n",
    "1 decimal corresponds to 11.1 km and zero to 111 km.\n",
    "\n",
    "The airport code data is composed of iso-region (i.e. US-PA) and coordinates data (longitude, latitude).\n",
    "\n",
    "In the end we'd like to relate the immigration data to temperatures.\n",
    "We cannot do this directly, so the goal is: \n",
    "1. read in immigration data, extract state.\n",
    "2. read in airport codes, extract latitude, longitude, round to zero decimals and extract state\n",
    "3. read in temperature data, extract latitude, longitude, round to zero decimals\n",
    "\n",
    "Using #3 we can create a helper table with columns: dayofmonth, month, lat, long, AvgTemp.\n",
    "The idea of this helper table is to determine an average temperature for a given day of month, month and state combination.\n",
    "\n",
    "Finally, we can link immigration data to average temperatures via the day of month and month (of the arrival dates) and the state.\n",
    "\n",
    "For a schema, see: <img src=\"https://i.imgur.com/jHBgMur.png\">\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "##### Below contains the data dictionary as well!\n",
    "\n",
    "1. Read in immigration data, temperature data, airport codes data\n",
    "2. Select relevant columns\n",
    "    1. immigration data: i94port|biryear|gender |airline|i94visa|i94addr|arrdate_dt|depdate_dt|arrdate_dayofmonth|arrdate_month|arrdate_year\n",
    "    1. temperature data: dayofmonth|month|lat|long|AvgTemp           \n",
    "    1. airport codes data: lat|long|state\n",
    "3. create fact and dimension tables\n",
    "    1. see image above for how to define them and their relationships\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create dimension tables\n",
    "\n",
    "Use .persist() on the smaller tables to optimize performance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def create_dim_state():\n",
    "    \"\"\"\n",
    "    Arguments: none. Return state dataframe.\n",
    "    \"\"\"\n",
    "    sc.setJobGroup(\"Read\", \"Read and transform dim_state\")\n",
    "    \n",
    "    return immigration_final_data.\\\n",
    "            select('state').\\\n",
    "            distinct().\\\n",
    "            withColumn(\"id_state\", monotonically_increasing_id())\n",
    "    \n",
    "def create_dim_time():\n",
    "    \"\"\"\n",
    "    Arguments: none. Return time dataframe.\n",
    "    \"\"\"\n",
    "    sc.setJobGroup(\"Read\", \"Read and transform dim_time\")\n",
    "    return immigration_final_data.\\\n",
    "            select(col('arrdate_dt').alias('date'), col('arrdate_dayofmonth').alias('day_of_month'), col('arrdate_month').alias('month'), col('arrdate_year').alias('year')).\\\n",
    "            distinct().\\\n",
    "            withColumn(\"id_time\", monotonically_increasing_id())\n",
    "    \n",
    "def create_dim_person():\n",
    "    \"\"\"\n",
    "    Arguments: none. Return person dataframe.\n",
    "    \"\"\"    \n",
    "    sc.setJobGroup(\"Read\", \"Read and transform dim_person\")\n",
    "    return immigration_final_data.\\\n",
    "            select('gender', 'biryear', 'id_imm').\\\n",
    "            withColumn(\"id_person\", monotonically_increasing_id())\n",
    "    \n",
    "def create_dim_ports():\n",
    "    \"\"\"\n",
    "    Arguments: none. Return ports dataframe.\n",
    "    \"\"\"    \n",
    "    sc.setJobGroup(\"Read\", \"Read and transform dim_ports\")\n",
    "    return immigration_final_data.\\\n",
    "            select('i94port').alias('port').\\\n",
    "            distinct().\\\n",
    "            withColumn(\"id_port\", monotonically_increasing_id())\n",
    "    \n",
    "def create_dim_airlines():\n",
    "    \"\"\"\n",
    "    Arguments: none. Return airline dataframe.\n",
    "    \"\"\"    \n",
    "    sc.setJobGroup(\"Read\", \"Read and transform dim_airlines\")\n",
    "    return immigration_final_data.\\\n",
    "            select('airline').\\\n",
    "            distinct().\\\n",
    "            withColumn(\"id_airline\", monotonically_increasing_id())\n",
    "    \n",
    "def create_fact_temp():\n",
    "    \"\"\"\n",
    "    Arguments: none. Return temperature dataframe.\n",
    "    \"\"\"    \n",
    "    sc.setJobGroup(\"Read\", \"Read and transform dim_temp\")\n",
    "    \n",
    "    return state_temp\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6775b6b994a64f6aa7b5d8b80158fd92",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "dim_state = create_dim_state()\n",
    "dim_state.persist()\n",
    "dim_state.limit(10).show(truncate=False)\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7a0739fadc4dce80e12761c21fd883",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+------------+\n",
      "|state|id_state    |\n",
      "+-----+------------+\n",
      "|LA   |77309411328 |\n",
      "|NJ   |137438953472|\n",
      "|OR   |257698037760|\n",
      "|RI   |403726925824|\n",
      "|KY   |420906795008|\n",
      "|WY   |420906795009|\n",
      "|NH   |438086664192|\n",
      "|MI   |463856467968|\n",
      "|NV   |472446402560|\n",
      "|MT   |575525617664|\n",
      "+-----+------------+"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "dim_time = create_dim_time()\n",
    "dim_time.persist()\n",
    "dim_time.limit(10).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775efdb34336415da28488348852e50c",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------------+-----+----+------------+\n",
      "|date      |day_of_month|month|year|id_time     |\n",
      "+----------+------------+-----+----+------------+\n",
      "|2016-04-13|13          |4    |2016|34359738368 |\n",
      "|2016-04-04|4           |4    |2016|51539607552 |\n",
      "|2016-05-27|27          |5    |2016|51539607553 |\n",
      "|2016-04-30|30          |4    |2016|77309411328 |\n",
      "|2016-05-17|17          |5    |2016|85899345920 |\n",
      "|2016-05-01|1           |5    |2016|146028888064|\n",
      "|2016-04-16|16          |4    |2016|137438953472|\n",
      "|2016-05-02|2           |5    |2016|163208757248|\n",
      "|2016-04-08|8           |4    |2016|154618822656|\n",
      "|2016-04-20|20          |4    |2016|369367187456|\n",
      "+----------+------------+-----+----+------------+"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "dim_person = create_dim_person()\n",
    "dim_person.persist()\n",
    "dim_person.limit(10).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49ee014e2aa4d6ebfe7379c443b7fbf",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+---------+\n",
      "|gender|biryear|id_imm|id_person|\n",
      "+------+-------+------+---------+\n",
      "|F     |1981.0 |0     |0        |\n",
      "|F     |1970.0 |1     |1        |\n",
      "|F     |1975.0 |2     |2        |\n",
      "|M     |1975.0 |3     |3        |\n",
      "|F     |1963.0 |4     |4        |\n",
      "|M     |1960.0 |5     |5        |\n",
      "|F     |1957.0 |6     |6        |\n",
      "|F     |1959.0 |7     |7        |\n",
      "|M     |1977.0 |8     |8        |\n",
      "|F     |1964.0 |9     |9        |\n",
      "+------+-------+------+---------+"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "dim_ports = create_dim_ports()\n",
    "dim_ports.persist()\n",
    "dim_ports.limit(10).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642d8f17288b4ff6b67b8125a973424c",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+------------+\n",
      "|i94port|id_port     |\n",
      "+-------+------------+\n",
      "|DNS    |8589934592  |\n",
      "|MOR    |8589934593  |\n",
      "|HEL    |8589934594  |\n",
      "|SNA    |34359738368 |\n",
      "|PTK    |34359738369 |\n",
      "|DLB    |68719476736 |\n",
      "|ABS    |68719476737 |\n",
      "|PVD    |103079215104|\n",
      "|MYR    |103079215105|\n",
      "|OAK    |111669149696|\n",
      "+-------+------------+"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "dim_airlines = create_dim_airlines()\n",
    "dim_airlines.persist()\n",
    "dim_airlines.limit(10).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112a1bea134f406eb4e6762d6bb02ce9",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-----------+\n",
      "|airline|id_airline |\n",
      "+-------+-----------+\n",
      "|DZ     |0          |\n",
      "|MM     |1          |\n",
      "|LT     |2          |\n",
      "|FI     |34359738368|\n",
      "|AZ     |34359738369|\n",
      "|R0E    |34359738370|\n",
      "|B01    |34359738371|\n",
      "|IC     |34359738372|\n",
      "|FYG    |51539607552|\n",
      "|743    |51539607553|\n",
      "+-------+-----------+"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "fact_temp = create_fact_temp()\n",
    "# fact_temp.show(444,truncate=False)\n",
    "print(f\"No of records in fact_temp: {fact_temp.count()}\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066b4b26791c47dea07f0122a60264dc",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of records in fact_temp: 444"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create fact table\n",
    "\n",
    "The fact_imm table should have 7 columns\n",
    "1. id_imm\n",
    "1. id_state\n",
    "1. id_time\n",
    "1. id_person\n",
    "1. id_port\n",
    "1. id_airline\n",
    "1. id_temp\n",
    "\n",
    "This is chosen so that we can all link the relevant immigration and temperature data together.\n",
    "In joining the data we perform left joins. Each entry in immigration_final_data should be preserved as not to lose track of immigration records.\n",
    "Also, some data is incomplete. For instance, the fact_temp table has 444 records total. This is suppressed due to not every state having a temperature defined in the dataset for each day of month and month combination. Making the input data complete would solve this problem.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def create_fact_imm():\n",
    "    \"\"\"\n",
    "    Arguments: none. Return fact_imm dataframe.\n",
    "    \"\"\"    \n",
    "    sc.setJobGroup(\"Read\", \"Read and transform fact_imm\")\n",
    "    \n",
    "    # we specifically perform left joins. we could transition to inner joins but some of the tables incomplete due to lacking information in the temperature/airport code table.\n",
    "    return immigration_final_data.\\\n",
    "            join(dim_time, [immigration_final_data.arrdate_dt == dim_time.date], \"left\").\\\n",
    "            join(dim_airlines, [immigration_final_data.airline == dim_airlines.airline], \"left\").\\\n",
    "            join(dim_ports, [immigration_final_data.i94port == dim_ports.i94port], \"left\").\\\n",
    "            join(dim_state, [immigration_final_data.state == dim_state.state], \"left\").\\\n",
    "            join(fact_temp, [immigration_final_data.arrdate_dayofmonth == fact_temp.dayofmonth, immigration_final_data.arrdate_month == fact_temp.month, immigration_final_data.state == fact_temp.state], \"left\").\\\n",
    "            join(dim_person, [immigration_final_data.id_imm == dim_person.id_imm], \"left\").\\\n",
    "            select(immigration_final_data.id_imm, dim_state.id_state, 'id_time', 'id_person', 'id_port', 'id_airline', 'id_temp')\n",
    "\n",
    "fact_imm = create_fact_imm()\n",
    "print(f\"No of records in fact_imm: {fact_imm.count()}\")\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5212c01849be4a30b2ed51c4594e809b",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of records in fact_imm: 5388905"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write the dimension/fact tables to S3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def writeToS3(displayName, destPath, writeMode, fmt, dataFrame):\n",
    "    \"\"\"\n",
    "    Given a display name displayName, write dataFrame to s3 path 'destPath' with writeMode either 'append' or overwrite\n",
    "    Use format fmt\n",
    "    Returns None \n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(f\"Writing {displayName} to {destPath} with format {fmt}, mode {writeMode}.\")\n",
    "\n",
    "    dataFrame.write.format(fmt).mode(writeMode).save(destPath)\n",
    "    \n",
    "    \n",
    "# write out dimension/fact tables to s3\n",
    "writeToS3('dim_state', f\"s3a://{s3_bucket}/capstone/processed/dim_state\", 'overwrite', 'parquet', dim_state)\n",
    "writeToS3('dim_time', f\"s3a://{s3_bucket}/capstone/processed/dim_time\", 'overwrite', 'parquet', dim_time)\n",
    "writeToS3('dim_person', f\"s3a://{s3_bucket}/capstone/processed/dim_person\", 'overwrite', 'parquet', dim_person)\n",
    "writeToS3('dim_airlines', f\"s3a://{s3_bucket}/capstone/processed/dim_airlines\", 'overwrite', 'parquet', dim_airlines)\n",
    "writeToS3('dim_ports', f\"s3a://{s3_bucket}/capstone/processed/dim_ports\", 'overwrite', 'parquet', dim_ports)\n",
    "\n",
    "writeToS3('fact_imm', f\"s3a://{s3_bucket}/capstone/processed/fact_imm\", 'overwrite', 'parquet', fact_imm)\n",
    "writeToS3('fact_temp', f\"s3a://{s3_bucket}/capstone/processed/fact_temp\", 'overwrite', 'parquet', fact_temp)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\n",
    "def recordCount(table_name):\n",
    "    \"\"\"\n",
    "    Given a dataframe table_name, return the number of records in it\n",
    "    \"\"\"\n",
    "    return table_name.count()\n",
    "\n",
    "def checkNumberOfRows(actual_count, expected_count):\n",
    "    \"\"\"\n",
    "    Given a number actual_count, compare to expected_count and raise ValueError exception if it differs.\n",
    "    \"\"\"\n",
    "    if actual_count != expected_count:\n",
    "        logging.error(f\"The number of records found is {actual_count}, differing from expected value {expected_count}\")\n",
    "        raise ValueError(f\"The number of records found is {actual_count}, differing from expected value {expected_count}\")\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6f64a9c4914ad1b9ca264f81a241ec",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# perform data quality checks\n",
    "spark.sparkContext.setJobGroup(\"DataQuality\", \"Counting number of records in tables\")\n",
    "\n",
    "expectedRowCount = {dim_state : 52, \n",
    "                    fact_temp : 444, \n",
    "                    dim_time : 61, \n",
    "                    fact_imm : 5388905,\n",
    "                    dim_airlines : 622,\n",
    "                    dim_person : 5388905,\n",
    "                    dim_ports : 314,}\n",
    "\n",
    "for obj in [dim_state, fact_temp, dim_time, fact_imm, dim_airlines, dim_person, dim_ports]:\n",
    "    print(f\"Evaluating {obj}\")\n",
    "    logging.info(f\"Evaluating {obj}\")\n",
    "    numRows = recordCount(obj)\n",
    "\n",
    "    checkNumberOfRows(numRows, expectedRowCount[obj])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f692dcd06fd4402c8928b138b8342da8",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluating DataFrame[state: string, id_state: bigint]\n",
      "Evaluating DataFrame[dayofmonth: int, month: int, state: string, avg_temp: double, id_temp: bigint]\n",
      "Evaluating DataFrame[date: date, day_of_month: int, month: int, year: int, id_time: bigint]\n",
      "Evaluating DataFrame[id_imm: bigint, id_state: bigint, id_time: bigint, id_person: bigint, id_port: bigint, id_airline: bigint, id_temp: bigint]\n",
      "Evaluating DataFrame[airline: string, id_airline: bigint]\n",
      "Evaluating DataFrame[gender: string, biryear: double, id_imm: bigint, id_person: bigint]\n",
      "Evaluating DataFrame[i94port: string, id_port: bigint]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "spark.sparkContext.setJobGroup(\"DataQuality\", \"Counting total number of distinct states\")\n",
    "dim_state.createOrReplaceTempView(\"state\")\n",
    "numDistinctStates = spark.sql(\"\"\"\n",
    "select count(distinct state) \n",
    "from state\n",
    "\"\"\")\n",
    "\n",
    "checkNumberOfRows(numDistinctStates.collect()[0]['count(DISTINCT state)'], len(valid_us_states) + 1)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635bfda239a94b27ad1aa70c756a079a",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "In this project we choose the combination of AWS S3 for fast & efficient storage with an AWS EMR cluster for efficient processing on large datasets.\n",
    "S3 is used as well to store the resulting analysis.\n",
    "\n",
    "As we have relatively static data (immigration data grows relatively slowly with one day at a time) this analysis was set up as a run-once thing. Meaning, we put the data somewhere, run a one-time analysis and proceed wit hthe results.\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "Temperature data currently is averaged out over many years so there's no need to update it often unless weather conditions across the US change a lot. \n",
    "\n",
    "Immigration data will have grown by a year after one year of waiting so a yearly frequency might be in order.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.<br>\n",
    "If the data was increased by 100x then I'd probably look into partitioning the data into smaller partitions. Furthermore, I would increase the number of nodes on the EMR cluster for faster processing. I'd also consider moving the data from S3 to Hadoop FS to optimize the I/O across the network.\n",
    "Currently we also process all information at once and in full. It will be more efficient to partition data by time and process only the new data (with the help of a workflow management system like Apache Airflow for instance). As the Uber case has shown (one of the topics that have been suggested to me) it probably will be more efficient as well to skip the transformation part in the ETL process and copy the bulk data unprocessed to S3. Then, copy the new bulk data to Hadoop and run the transformations there.\n",
    "The currently chosen partition key of month, day of month is a partition by time so if there is a 100x increase of data the partitions each will get 100x larger as well. Therefore an additional subdivision might come in handy. Perhaps partitioning by state (seeing as we group by state often) might be useful.\n",
    "Lastly, I'd more strictly enforce a schema to ensure data quality.\n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.<br>\n",
    "I'd create an Airflow workflow to easily schedule this task. Furthermore I'd process the history once and rewrite the code such that only the daily data added would be processed.\n",
    " \n",
    " * The database needed to be accessed by 100+ people.<br>\n",
    "If the database needed to be accessed by 100+ people I'd run the analysis once and cache the results somewhere and serve the cache. As the setup currently is now, there is no user input that could change the output. This would reflect the heavy-read scenario which was described in the feedback.\n",
    "Generally, when optimizing for heavy reads one should make sure as few operations as possible are needed to serve the data. In this case I'd make sure the data would be horizontally scalable so I'd look at the proper partition/clustering keys and perhaps move the data into something like Apache Cassandra. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis example\n",
    "\n",
    "Find out what state has the most tourists per month and what is the temperature like at the destination?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "sc.setJobGroup(\"Analysis\", \"Counting number of tourists per month and the average temperature therein\")\n",
    "\n",
    "fact_imm.createOrReplaceTempView(\"fact_imm\")\n",
    "fact_temp.createOrReplaceTempView(\"fact_temp\")\n",
    "\n",
    "analysis1 = spark.sql(\"\"\"\n",
    "select ft.month, ft.state, avg(ft.avg_temp) as avg_temp, count(fi.id_imm) as tourist_num \n",
    "from fact_imm fi\n",
    "join fact_temp ft on (fi.id_temp = ft.id_temp)\n",
    "group by month, state\n",
    "order by tourist_num desc\n",
    "\"\"\")\n",
    "analysis1.limit(10).show(truncate=False)\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6983bc9138149449f230764f28ffa18",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+-----+------------------+-----------+\n",
      "|month|state|avg_temp          |tourist_num|\n",
      "+-----+-----+------------------+-----------+\n",
      "|4    |FL   |20.952401390146782|22934      |\n",
      "|5    |FL   |24.05730552869499 |18995      |\n",
      "|4    |NY   |6.426848223164618 |17658      |\n",
      "|4    |CA   |12.953549933932452|13897      |\n",
      "|5    |NY   |13.02698528746373 |12758      |\n",
      "|5    |CA   |16.180092082676367|12066      |\n",
      "|5    |NV   |21.31980813953488 |4081       |\n",
      "|4    |TX   |18.75916808141302 |3873       |\n",
      "|5    |TX   |22.84649000785298 |3000       |\n",
      "|4    |IL   |11.008175343667743|2622       |\n",
      "+-----+-----+------------------+-----------+"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}